{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# FrameWork utilizado --> Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#\n",
    "import tqdm\n",
    "\n",
    "# Pegando o Dataset pelo Keras\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_X, train_y), (test_X, test_y) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader\n",
    "O data Loader será utilizado para poder pegar os dados aleatoriamente e controlar o \"Batch Size\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST(Dataset):\n",
    "    \n",
    "    def __init__(self,X,y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.X[idx],self.y[idx]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(MNIST(train_X, train_y), batch_size=256)\n",
    "test_dl = DataLoader(MNIST(test_X, test_y), batch_size= 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criando o Modelo de MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MLP_Model(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MLP_Model,self).__init__()\n",
    "        \n",
    "        # nn.Linear(a,b) --> a entrada, b mapeia ?\n",
    "        # O primeiro neurônio tem 4 no parênteses pois o data set tem 4 caracteristicas no X,  \n",
    "        \n",
    "        self.L1 =  nn.Linear(4,32)\n",
    "        self.L2 =  nn.Linear(32,16)\n",
    "        self.L3 =  nn.Linear(16,8)\n",
    "        self.L4 =  nn.Linear(8,3) \n",
    "        \n",
    "        self.A1 =  nn.ReLU()\n",
    "        \n",
    "        # No segundo, receberá o tanto que saiu na layer passada, logo:\n",
    "        # nn.Linear(c,d) --> c == a, d == quantidasde que sera y previsto, ex: probabilidade de ser cada uma das flores\n",
    "        \n",
    "        self.L2 =  nn.Linear(32,16)\n",
    "        self.L3 =  nn.Linear(16,8)\n",
    "        self.L4 =  nn.Linear(8,3) \n",
    "        \n",
    "        # SoftMax é uma função para classificação\n",
    "\n",
    "        self.A2 = nn.Softmax()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        \n",
    "        \n",
    "        x = self.L1(x)\n",
    "        # Aqui estou transformando o vetor inicial de 4 em um vetor de \n",
    "        x = self.A1(x)\n",
    "        # Escalando o vetor via função sigmoid, outra possivel é a função reLU --> ler mais sobre\n",
    "        x = self.L2(x)\n",
    "        x = self.A1(x)\n",
    "        x = self.L3(x)\n",
    "        x = self.A1(x)\n",
    "        x = self.L4(x)\n",
    "        # Aqui estou transformando o vetor modificado de tamanho 5 em um vetor de 3\n",
    "        x = self.A2(x)\n",
    "        \n",
    "        \n",
    "        \n",
    "        return x       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinando o modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training( N_Epochs, model, loss_fn, opt  ):\n",
    "    \n",
    "    loss_list = []\n",
    "    \n",
    "    for epoch in tqdm(range(N_Epochs+1)):\n",
    "        for xb, yb in train_dl:\n",
    "            \n",
    "            y_pred = model(xb.float())\n",
    "            loss   = loss_fn(y_pred, yb.long())\n",
    "            \n",
    "            \n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "        \n",
    "        #print(epoch, \"loss=\", loss.item())\n",
    "        loss_list.append(loss.item())\n",
    "            \n",
    "    \n",
    "    plt.figure(figsize = (14,6))\n",
    "    plt.title(\"cost decay\")\n",
    "    plt.plot(loss_list)\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"cost\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
